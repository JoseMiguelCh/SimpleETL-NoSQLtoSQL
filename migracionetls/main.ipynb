{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97a60f5-e4a3-41c4-83e8-df59a3019d95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d2c9fa-ea33-4001-850d-c2da2b4544d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from migracionetls.data_extraction.extract import extract_data\n",
    "from migracionetls.data_transformation.transform import transform_data\n",
    "from migracionetls.data_loading.load import load_data\n",
    "from migracionetls.spark_utils import initialize_spark\n",
    "from migracionetls.schemas import (codigos_epc_hab_map, lista_usuarios_hab_map, \n",
    "    negaciones_paso_hab_map, pasos_hab_map, pruebas_hab_map)\n",
    "\n",
    "load_dotenv()\n",
    "ENDPOINT = os.getenv(\"ENDPOINT\")\n",
    "MASTER_KEY = os.getenv(\"MASTER_KEY\")\n",
    "\n",
    "CONTAINERS_TO_EXTRACT = {\n",
    "    'CodigosEPCHab': codigos_epc_hab_map,\n",
    "    'ListaUsuariosHab': lista_usuarios_hab_map,\n",
    "    'NegacionesPasoHab': negaciones_paso_hab_map,\n",
    "    'PasosHab': pasos_hab_map,\n",
    "    'PruebasHab': pruebas_hab_map,\n",
    "}\n",
    "\n",
    "def main():\n",
    "    logging.info(\"------ Initializing spark session -------\")\n",
    "    spark = initialize_spark(ENDPOINT, MASTER_KEY)\n",
    "    for container in CONTAINERS_TO_EXTRACT:\n",
    "        logging.info(f\"------ Starting extraction for {container} -------\")\n",
    "        logging.info(\"------ Extracting data from cosmos db -------\")\n",
    "        df = extract_data(spark, container, CONTAINERS_TO_EXTRACT[container]['schema'])\n",
    "        logging.info(\"------ Transforming data -------\")\n",
    "        transformed_data = transform_data(spark, df, CONTAINERS_TO_EXTRACT[container])\n",
    "        logging.info(\"------ Loading data into PSQL -------\")\n",
    "        [load_data(source, target) for source, target in transformed_data]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
